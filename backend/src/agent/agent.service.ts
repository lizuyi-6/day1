import { Injectable, Logger } from '@nestjs/common';
import { KnowledgeService } from '../knowledge/knowledge.service';
import { WorkflowService } from '../workflow/workflow.service';
import { SessionService } from '../session/session.service';
import { Parser } from 'expr-eval'; // å®‰å…¨çš„è¡¨è¾¾å¼è§£æå™¨
import { ChatOpenAI } from '@langchain/openai';
import { HumanMessage, SystemMessage } from '@langchain/core/messages';
import { Observable } from 'rxjs';

@Injectable()
export class AgentService {
  private readonly logger = new Logger(AgentService.name);
  private llm: ChatOpenAI;

  constructor(
    private readonly knowledgeService: KnowledgeService,
    private readonly workflowService: WorkflowService,
    private readonly sessionService: SessionService,
  ) {
    // åˆå§‹åŒ– Qwen LLM
    const apiKey = process.env.OPENAI_API_KEY;
    const baseURL = process.env.OPENAI_BASE_URL;
    const modelName = process.env.LLM_MODEL || 'qwen-flash';

    if (apiKey) {
      const config: any = {
        modelName: modelName,
        temperature: 0.7,
      };

      if (baseURL) {
        config.configuration = {
          baseURL: baseURL,
        };
      }

      this.llm = new ChatOpenAI(config);
      this.logger.log(
        `âœ… LLM initialized: ${modelName} @ ${baseURL || 'OpenAI'}`,
      );
    } else {
      this.logger.warn(
        'âš ï¸  OPENAI_API_KEY not configured, using mock responses',
      );
    }
  }

  async executeWorkflow(workflowId: string, inputMessage: string) {
    this.logger.log(
      `Executing workflow ${workflowId} with input: ${inputMessage}`,
    );

    // 1. Fetch Workflow
    const workflow = await this.workflowService.findOne(workflowId);
    if (!workflow || !workflow.graphData) {
      throw new Error('Workflow not found or empty');
    }

    // 2. Parse Graph (Simple Linear Execution for Demo)
    // In a real system, we'd do topological sort on nodes/edges.
    // Here we just find the 'start' node and follow edges.

    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    const graph: any = workflow.graphData;
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    let nodes: any[] = [];
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    let edges: any[] = [];

    // Support Vue Flow structure (nodes/edges) OR X6 structure (cells)
    if (graph.nodes && graph.edges) {
      nodes = graph.nodes;
      edges = graph.edges;
    } else if (graph.cells) {
      // Fallback to X6 structure

      nodes = graph.cells.filter((c: any) => c.shape !== 'edge');

      edges = graph.cells.filter((c: any) => c.shape === 'edge');
    }

    let currentNode = nodes.find((n: any) => n.data?.type === 'start');
    if (!currentNode) {
      return { response: 'ERROR: No START node found.' };
    }

    let executionResult = `[Workflow Start]\nInput: ${inputMessage}\n`;

    // Max steps to prevent infinite loops
    let steps = 0;
    while (currentNode && steps < 20) {
      // Vue Flow uses 'label', X6 uses attrs.label.text
      const label =
        currentNode.label || currentNode.attrs?.label?.text || 'Node';
      executionResult += `> Step ${steps + 1}: Executing ${label} (${currentNode.data.type})\n`;

      let nextEdgeLabel = null; // For branching

      // Execute Node Logic
      if (currentNode.data.type === 'action') {
        // Mock LLM Action or Script
        executionResult += `  Action: Processing "${currentNode.data.prompt || 'default'}"...\n`;
        // In real agent, this would call LLMService
        executionResult += `  Output: [Success]\n`;
      } else if (currentNode.data.type === 'condition') {
        const expression = currentNode.data.expression || 'true';
        executionResult += `  Condition: Evaluating "${expression}"...\n`;

        try {
          // ä½¿ç”¨å®‰å…¨çš„è¡¨è¾¾å¼è§£æå™¨
          const parser = new Parser();

          // åˆ›å»ºå®‰å…¨çš„ä¸Šä¸‹æ–‡
          const context: Record<string, any> = {
            input: inputMessage,
            length: inputMessage?.length || 0,
          };

          const result = parser.parse(expression).evaluate(context);
          executionResult += `  Result: ${result}\n`;

          // If result is true, look for 'true' or 'yes' edge, else 'false' or 'no'
          // For simplicity in this demo, we assume Condition nodes have two outgoing edges: "YES" and "NO" (labels)
          nextEdgeLabel = result ? 'YES' : 'NO';
        } catch (e) {
          executionResult += `  Error evaluating condition: ${e.message}\n`;
          nextEdgeLabel = 'NO'; // Fallback
        }
      }

      // Find next node
      // Vue Flow edge: source/target are Ids. X6 edge: source.cell/target.cell are Ids

      const outgoingEdges = edges.filter((e: any) => {
        const sourceId = e.source?.cell || e.source;
        return sourceId === currentNode.id;
      });

      let edge;
      if (outgoingEdges.length === 0) {
        executionResult += `[Workflow End] No outgoing edge.\n`;
        break;
      } else if (outgoingEdges.length === 1) {
        edge = outgoingEdges[0];
      } else {
        // Branching logic
        if (nextEdgeLabel) {
          // Find edge with matching label (case insensitive)
          // Vue Flow: label. X6: labels[0].attrs.text.text

          edge = outgoingEdges.find((e: any) => {
            const edgeText = e.label || e.labels?.[0]?.attrs?.text?.text;
            return edgeText && edgeText.toUpperCase() === nextEdgeLabel;
          });

          if (!edge) {
            // Fallback to first if no label match
            executionResult += `  Warning: No edge found for branch '${nextEdgeLabel}', taking first path.\n`;
            edge = outgoingEdges[0];
          }
        } else {
          // No condition branch implied, just take first
          edge = outgoingEdges[0];
        }
      }

      if (!edge) {
        executionResult += `[Workflow End] Path dead end.\n`;
        break;
      }

      const targetId = edge.target?.cell || edge.target;
      currentNode = nodes.find((n: any) => n.id === targetId);
      steps++;
    }

    return {
      response: executionResult,
      status: 'completed',
    };
  }

  async chat(message: string, sessionId?: string) {
    // è·å–æˆ–åˆ›å»º Session
    const session = sessionId
      ? await this.sessionService.getOrCreateSession(sessionId)
      : await this.sessionService.createSession();

    // ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
    await this.sessionService.addMessage(session.sessionId, 'user', message);

    if (message.startsWith('/run ') || message.startsWith('Run ')) {
      const workflowName = message.replace(/^\/run |^Run /i, '').trim();
      const allWorkflows = await this.workflowService.findAll();
      const workflow = allWorkflows.find((w) => w.name.includes(workflowName));

      if (workflow) {
        const result = await this.executeWorkflow(workflow.id, message);
        // ä¿å­˜åŠ©æ‰‹å“åº”
        await this.sessionService.addMessage(
          session.sessionId,
          'assistant',
          result.response,
        );
        return { ...result, sessionId: session.sessionId };
      } else {
        const errorMsg = `Workflow with name containing "${workflowName}" not found.`;
        await this.sessionService.addMessage(
          session.sessionId,
          'assistant',
          errorMsg,
        );
        return { response: errorMsg, sessionId: session.sessionId };
      }
    }

    // è·å–å¯¹è¯å†å²ï¼ˆæœ€è¿‘ 5 è½®ï¼‰
    const history = await this.sessionService.getConversationHistory(
      session.sessionId,
      10,
    );
    const historyContext = history
      .filter((m) => m.role !== 'system')
      .map((m) => `${m.role}: ${m.content}`)
      .join('\n');

    // RAG: æ£€ç´¢çŸ¥è¯†åº“
    const docs = await this.knowledgeService.search(message, 3);
    const context = docs.map((d) => d.content).join('\n\n---\n\n');

    if (this.llm) {
      try {
        const systemPrompt = `ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼ŒåŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

çŸ¥è¯†åº“å†…å®¹ï¼š
${context || 'ï¼ˆæš‚æ— çŸ¥è¯†åº“å†…å®¹ï¼‰'}

å¯¹è¯å†å²ï¼š
${historyContext || 'ï¼ˆæš‚æ— å†å²è®°å½•ï¼‰'}

è¯·æ ¹æ®çŸ¥è¯†åº“å†…å®¹å’Œå¯¹è¯å†å²ç»™å‡ºå‡†ç¡®ã€ç®€æ´çš„å›ç­”ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ã€‚

è¦æ±‚ï¼š
1. åŸºäºçŸ¥è¯†åº“å†…å®¹å›ç­”
2. å‚è€ƒå¯¹è¯å†å²è¿›è¡Œå¤šè½®å¯¹è¯
3. å›ç­”è¦ç®€æ´ã€å‡†ç¡®
4. å¯ä»¥é€‚å½“å¼•ç”¨çŸ¥è¯†åº“ä¸­çš„å…·ä½“å†…å®¹`;

        const response = await this.llm.invoke([
          new SystemMessage(systemPrompt),
          new HumanMessage(message),
        ]);

        const responseText = response.content as string;

        // ä¿å­˜åŠ©æ‰‹å“åº”
        await this.sessionService.addMessage(
          session.sessionId,
          'assistant',
          responseText,
        );

        return {
          response: responseText,
          sources: docs.map((d) => d.fileName),
          sessionId: session.sessionId,
        };
      } catch (error) {
        this.logger.error(`LLM è°ƒç”¨å¤±è´¥: ${error.message}`);
        const errorMsg = `âš ï¸ LLM æœåŠ¡æš‚æ—¶ä¸å¯ç”¨\n\nåŸºäºçŸ¥è¯†åº“æ£€ç´¢åˆ° ${docs.length} ä¸ªç›¸å…³ç‰‡æ®µï¼š\n${context || 'æ— ç›¸å…³å†…å®¹'}`;
        await this.sessionService.addMessage(
          session.sessionId,
          'assistant',
          errorMsg,
        );
        return {
          response: errorMsg,
          sources: docs.map((d) => d.fileName),
          sessionId: session.sessionId,
        };
      }
    }

    // Fallback
    const fallbackMsg = `[RAG æ£€ç´¢ç»“æœ]\n\næ‰¾åˆ° ${docs.length} ä¸ªç›¸å…³çŸ¥è¯†åº“ç‰‡æ®µï¼š\n\n${context || 'æš‚æ— ç›¸å…³å†…å®¹'}\n\nğŸ’¡ æç¤ºï¼šé…ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡ä»¥å¯ç”¨çœŸå®çš„ LLM å›ç­”ç”Ÿæˆ`;
    await this.sessionService.addMessage(
      session.sessionId,
      'assistant',
      fallbackMsg,
    );
    return {
      response: fallbackMsg,
      sources: docs.map((d) => d.fileName),
      sessionId: session.sessionId,
    };
  }

  // æµå¼ä¼ è¾“æ–¹æ³•ï¼ˆè¿”å› Observableï¼‰
  chatStream(message: string, sessionId?: string): Observable<MessageEvent> {
    return new Observable<MessageEvent>((subscriber) => {
      (async () => {
        try {
          // è·å–æˆ–åˆ›å»º Session
          const session = sessionId
            ? await this.sessionService.getOrCreateSession(sessionId)
            : await this.sessionService.createSession();

          // ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
          await this.sessionService.addMessage(
            session.sessionId,
            'user',
            message,
          );

          // RAG æ£€ç´¢
          const docs = await this.knowledgeService.search(message, 3);
          const context = docs.map((d) => d.content).join('\n\n---\n\n');

          // è·å–å†å²
          const history = await this.sessionService.getConversationHistory(
            session.sessionId,
            10,
          );
          const historyContext = history
            .filter((m) => m.role !== 'system')
            .map((m) => `${m.role}: ${m.content}`)
            .join('\n');

          const systemPrompt = `ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼ŒåŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

çŸ¥è¯†åº“å†…å®¹ï¼š
${context || 'ï¼ˆæš‚æ— çŸ¥è¯†åº“å†…å®¹ï¼‰'}

å¯¹è¯å†å²ï¼š
${historyContext || 'ï¼ˆæš‚æ— å†å²è®°å½•ï¼‰'}

è¯·æ ¹æ®çŸ¥è¯†åº“å†…å®¹å’Œå¯¹è¯å†å²ç»™å‡ºå‡†ç¡®ã€ç®€æ´çš„å›ç­”ã€‚`;

          if (this.llm) {
            // ä½¿ç”¨æµå¼ API
            const stream = await this.llm.stream([
              new SystemMessage(systemPrompt),
              new HumanMessage(message),
            ]);

            let fullResponse = '';

            for await (const chunk of stream) {
              const content = chunk.content as string;
              fullResponse += content;

              // å‘é€æµå¼äº‹ä»¶
              subscriber.next({
                data: JSON.stringify({
                  type: 'token',
                  content: content,
                  done: false,
                }),
              } as MessageEvent);
            }

            // ä¿å­˜å®Œæ•´å“åº”
            await this.sessionService.addMessage(
              session.sessionId,
              'assistant',
              fullResponse,
            );

            // å‘é€å®Œæˆäº‹ä»¶
            subscriber.next({
              data: JSON.stringify({
                type: 'done',
                content: '',
                response: fullResponse,
                sources: docs.map((d) => d.fileName),
                sessionId: session.sessionId,
                done: true,
              }),
            } as MessageEvent);

            subscriber.complete();
          } else {
            // Fallback without streaming
            const fallbackMsg = 'LLM æœªé…ç½®ï¼Œæ— æ³•ä½¿ç”¨æµå¼ä¼ è¾“';
            subscriber.error(new Error(fallbackMsg));
          }
        } catch (error) {
          this.logger.error(`Stream error: ${error.message}`);
          subscriber.error(error);
        }
      })();
    });
  }
}
