import { Injectable, Logger } from '@nestjs/common';
import { KnowledgeService } from '../knowledge/knowledge.service';
import { WorkflowService } from '../workflow/workflow.service';
import { SessionService } from '../session/session.service';
import { Parser } from 'expr-eval'; // å®‰å…¨çš„è¡¨è¾¾å¼è§£æå™¨
import { ChatOpenAI } from '@langchain/openai';
import { HumanMessage, SystemMessage } from '@langchain/core/messages';
import { Observable } from 'rxjs';
import { NodeRegistry } from '../workflow/nodes/node-registry';

@Injectable()
export class AgentService {
  private readonly logger = new Logger(AgentService.name);
  private llm: ChatOpenAI;

  constructor(
    private readonly knowledgeService: KnowledgeService,
    private readonly workflowService: WorkflowService,
    private readonly sessionService: SessionService,
    private readonly nodeRegistry: NodeRegistry,
  ) {
    // åˆå§‹åŒ– Qwen LLM
    // Demo Override: Force all LLM nodes to use Qwen
    const apiKey = 'sk-9dd62d22ea0b439eb96f6800d6c7749a';
    const baseURL = 'https://dashscope.aliyuncs.com/compatible-mode/v1';
    const modelName = 'qwen-flash';
    
    // const apiKey = process.env.OPENAI_API_KEY;
    // const baseURL = process.env.OPENAI_BASE_URL;
    // const modelName = process.env.LLM_MODEL || 'qwen-flash';

    if (apiKey) {
      const config: any = {
        modelName: modelName,
        temperature: 0.7,
      };

      if (baseURL) {
        config.configuration = {
          baseURL: baseURL,
        };
      }

      this.llm = new ChatOpenAI(config);
      this.logger.log(
        `âœ… LLM initialized: ${modelName} @ ${baseURL || 'OpenAI'}`,
      );
    } else {
      this.logger.warn(
        'âš ï¸  OPENAI_API_KEY not configured, using mock responses',
      );
    }
  }

  async executeWorkflow(workflowId: string, inputMessage: string) {
    this.logger.log(
      `Executing workflow ${workflowId} with input: ${inputMessage}`,
    );

    // 1. Fetch Workflow
    const workflow = await this.workflowService.findOne(workflowId);
    if (!workflow || !workflow.graphData) {
      throw new Error('Workflow not found or empty');
    }

    // 2. Parse Graph
    const graph: any = workflow.graphData;
    let nodes: any[] = [];
    let edges: any[] = [];

    // Support Vue Flow structure (nodes/edges) OR X6 structure (cells)
    if (graph.nodes && graph.edges) {
      nodes = graph.nodes;
      edges = graph.edges;
    } else if (graph.cells) {
      nodes = graph.cells.filter((c: any) => c.shape !== 'edge');
      edges = graph.cells.filter((c: any) => c.shape === 'edge');
    }

    let currentNode = nodes.find((n: any) => n.data?.type === 'start');
    if (!currentNode) {
      return { response: 'ERROR: No START node found.' };
    }

    let executionResult = '';
    let nodeOutputs: Record<string, Record<string, any>> = {};
    let lastNodeOutput: Record<string, any> = { input: inputMessage };

    // Max steps to prevent infinite loops
    let steps = 0;
    while (currentNode && steps < 50) {
      const nodeType = currentNode.data?.type || 'unknown';
      const nodeId = currentNode.id;

      this.logger.log(`Executing node: ${nodeType} (${nodeId})`);

      // Execute the node using NodeRegistry
      if (nodeType === 'start' || nodeType === 'end') {
        // Start and End nodes just pass through the input
        executionResult += `[${nodeType.toUpperCase()}]\n`;
        nodeOutputs[nodeId] = lastNodeOutput;
      } else {
        // Get node from registry
        const node = this.nodeRegistry.getNode(nodeType);

        if (!node) {
          executionResult += `[ERROR] Unknown node type: ${nodeType}\n`;
          this.logger.warn(`Unknown node type: ${nodeType}, skipping...`);
        } else {
          try {
            // Merge node data with last node output as inputs
            const inputs = {
              ...currentNode.data,
              ...lastNodeOutput,
            };

            const context: any = {
              workflowId,
              nodeId,
              variables: currentNode.data || {},
              nodeOutputs,
            };

            this.logger.log(`Executing ${nodeType} node with inputs:`, Object.keys(inputs));

            const output = await node.execute(inputs, context);
            nodeOutputs[nodeId] = output;
            lastNodeOutput = output;

            const outputSummary = JSON.stringify(output)
              .substring(0, 100)
              .replace(/\n/g, ' ');

            executionResult += `> ${nodeType.toUpperCase()}: ${outputSummary}...\n`;
            this.logger.log(`${nodeType} node output:`, output);
          } catch (error: any) {
            executionResult += `[ERROR] ${nodeType} failed: ${error.message}\n`;
            this.logger.error(`Node execution failed:`, error.message);
            return {
              response: executionResult,
              status: 'error',
              error: error.message,
            };
          }
        }
      }

      // Find next node
      const outgoingEdges = edges.filter((e: any) => {
        const sourceId = e.source?.cell || e.source;
        return sourceId === currentNode.id;
      });

      if (outgoingEdges.length === 0) {
        executionResult += `\n[Workflow Complete]\n`;
        break;
      }

      // For simplicity, take the first outgoing edge
      const edge = outgoingEdges[0];
      const targetId = edge.target?.cell || edge.target;
      currentNode = nodes.find((n: any) => n.id === targetId);

      if (!currentNode) {
        executionResult += `\n[Workflow End] Next node not found.\n`;
        break;
      }

      steps++;
    }

    // Try to extract the final meaningful output
    let finalOutput = executionResult;
    if (lastNodeOutput.result || lastNodeOutput.response || lastNodeOutput.text || lastNodeOutput.content) {
      finalOutput = lastNodeOutput.result || lastNodeOutput.response || lastNodeOutput.text || lastNodeOutput.content || executionResult;
    }

    return {
      response: finalOutput,
      status: 'completed',
      nodeOutputs,
    };
  }

  async chat(message: string, sessionId?: string, browserId?: string) {
    // è·å–æˆ–åˆ›å»º Session
    const session = sessionId
      ? await this.sessionService.getOrCreateSession(sessionId)
      : await this.sessionService.createSession();

    // ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
    await this.sessionService.addMessage(session.sessionId, 'user', message);

    if (message.startsWith('/run ') || message.startsWith('Run ')) {
      const workflowName = message.replace(/^\/run |^Run /i, '').trim();
      const allWorkflows = await this.workflowService.findAll();
      const workflow = allWorkflows.items.find((w) =>
        w.name.includes(workflowName),
      );

      if (workflow) {
        const result = await this.executeWorkflow(workflow.id, message);
        // ä¿å­˜åŠ©æ‰‹å“åº”
        await this.sessionService.addMessage(
          session.sessionId,
          'assistant',
          result.response,
        );
        return { ...result, sessionId: session.sessionId };
      } else {
        const errorMsg = `Workflow with name containing "${workflowName}" not found.`;
        await this.sessionService.addMessage(
          session.sessionId,
          'assistant',
          errorMsg,
        );
        return { response: errorMsg, sessionId: session.sessionId };
      }
    }

    // è·å–å¯¹è¯å†å²ï¼ˆæœ€è¿‘ 5 è½®ï¼‰
    const history = await this.sessionService.getConversationHistory(
      session.sessionId,
      10,
    );
    const historyContext = history
      .filter((m) => m.role !== 'system')
      .map((m) => `${m.role}: ${m.content}`)
      .join('\n');

    // RAG: æ£€ç´¢çŸ¥è¯†åº“
    const docs = await this.knowledgeService.search(message, browserId || 'anonymous', 3);
    const context = docs.map((d) => d.content).join('\n\n---\n\n');

    if (this.llm) {
      try {
        const systemPrompt = `ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼ŒåŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

çŸ¥è¯†åº“å†…å®¹ï¼š
${context || 'ï¼ˆæš‚æ— çŸ¥è¯†åº“å†…å®¹ï¼‰'}

å¯¹è¯å†å²ï¼š
${historyContext || 'ï¼ˆæš‚æ— å†å²è®°å½•ï¼‰'}

è¯·æ ¹æ®çŸ¥è¯†åº“å†…å®¹å’Œå¯¹è¯å†å²ç»™å‡ºå‡†ç¡®ã€ç®€æ´çš„å›ç­”ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ã€‚

è¦æ±‚ï¼š
1. åŸºäºçŸ¥è¯†åº“å†…å®¹å›ç­”
2. å‚è€ƒå¯¹è¯å†å²è¿›è¡Œå¤šè½®å¯¹è¯
3. å›ç­”è¦ç®€æ´ã€å‡†ç¡®
4. å¯ä»¥é€‚å½“å¼•ç”¨çŸ¥è¯†åº“ä¸­çš„å…·ä½“å†…å®¹`;

        const response = await this.llm.invoke([
          new SystemMessage(systemPrompt),
          new HumanMessage(message),
        ]);

        const responseText = response.content as string;

        // ä¿å­˜åŠ©æ‰‹å“åº”
        await this.sessionService.addMessage(
          session.sessionId,
          'assistant',
          responseText,
        );

        return {
          response: responseText,
          sources: docs.map((d) => d.fileName),
          sessionId: session.sessionId,
        };
      } catch (error) {
        this.logger.error(`LLM è°ƒç”¨å¤±è´¥: ${error.message}`);
        const errorMsg = `âš ï¸ LLM æœåŠ¡æš‚æ—¶ä¸å¯ç”¨\n\nåŸºäºçŸ¥è¯†åº“æ£€ç´¢åˆ° ${docs.length} ä¸ªç›¸å…³ç‰‡æ®µï¼š\n${context || 'æ— ç›¸å…³å†…å®¹'}`;
        await this.sessionService.addMessage(
          session.sessionId,
          'assistant',
          errorMsg,
        );
        return {
          response: errorMsg,
          sources: docs.map((d) => d.fileName),
          sessionId: session.sessionId,
        };
      }
    }

    // Fallback
    const fallbackMsg = `[RAG æ£€ç´¢ç»“æœ]\n\næ‰¾åˆ° ${docs.length} ä¸ªç›¸å…³çŸ¥è¯†åº“ç‰‡æ®µï¼š\n\n${context || 'æš‚æ— ç›¸å…³å†…å®¹'}\n\nğŸ’¡ æç¤ºï¼šé…ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡ä»¥å¯ç”¨çœŸå®çš„ LLM å›ç­”ç”Ÿæˆ`;
    await this.sessionService.addMessage(
      session.sessionId,
      'assistant',
      fallbackMsg,
    );
    return {
      response: fallbackMsg,
      sources: docs.map((d) => d.fileName),
      sessionId: session.sessionId,
    };
  }

  // æµå¼ä¼ è¾“æ–¹æ³•ï¼ˆè¿”å› Observableï¼‰
  chatStream(message: string, sessionId?: string, browserId?: string): Observable<MessageEvent> {
    return new Observable<MessageEvent>((subscriber) => {
      (async () => {
        try {
          // è·å–æˆ–åˆ›å»º Session
          const session = sessionId
            ? await this.sessionService.getOrCreateSession(sessionId)
            : await this.sessionService.createSession();

          // ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
          await this.sessionService.addMessage(
            session.sessionId,
            'user',
            message,
          );

          // RAG æ£€ç´¢
          const docs = await this.knowledgeService.search(message, browserId || 'anonymous', 3);
          const context = docs.map((d) => d.content).join('\n\n---\n\n');

          // è·å–å†å²
          const history = await this.sessionService.getConversationHistory(
            session.sessionId,
            10,
          );
          const historyContext = history
            .filter((m) => m.role !== 'system')
            .map((m) => `${m.role}: ${m.content}`)
            .join('\n');

          const systemPrompt = `ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼ŒåŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

çŸ¥è¯†åº“å†…å®¹ï¼š
${context || 'ï¼ˆæš‚æ— çŸ¥è¯†åº“å†…å®¹ï¼‰'}

å¯¹è¯å†å²ï¼š
${historyContext || 'ï¼ˆæš‚æ— å†å²è®°å½•ï¼‰'}

è¯·æ ¹æ®çŸ¥è¯†åº“å†…å®¹å’Œå¯¹è¯å†å²ç»™å‡ºå‡†ç¡®ã€ç®€æ´çš„å›ç­”ã€‚`;

          if (this.llm) {
            // ä½¿ç”¨æµå¼ API
            const stream = await this.llm.stream([
              new SystemMessage(systemPrompt),
              new HumanMessage(message),
            ]);

            let fullResponse = '';

            for await (const chunk of stream) {
              const content = chunk.content as string;
              fullResponse += content;

              // å‘é€æµå¼äº‹ä»¶
              subscriber.next({
                data: JSON.stringify({
                  type: 'token',
                  content: content,
                  done: false,
                }),
              } as MessageEvent);
            }

            // ä¿å­˜å®Œæ•´å“åº”
            await this.sessionService.addMessage(
              session.sessionId,
              'assistant',
              fullResponse,
            );

            // å‘é€å®Œæˆäº‹ä»¶
            subscriber.next({
              data: JSON.stringify({
                type: 'done',
                content: '',
                response: fullResponse,
                sources: docs.map((d) => d.fileName),
                sessionId: session.sessionId,
                done: true,
              }),
            } as MessageEvent);

            subscriber.complete();
          } else {
            // Fallback without streaming
            const fallbackMsg = 'LLM æœªé…ç½®ï¼Œæ— æ³•ä½¿ç”¨æµå¼ä¼ è¾“';
            subscriber.error(new Error(fallbackMsg));
          }
        } catch (error) {
          this.logger.error(`Stream error: ${error.message}`);
          subscriber.error(error);
        }
      })();
    });
  }
}
